{
    "nbformat": 4, 
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Machine learning optimization using the cognitive assistant function\n\nThis notebook demonstrates the use of the cognitive assistant that is part of Jupyter Notebook running a Python kernel. In this sample, you will use cognitive assistant function to optimize machine learning piplelines for a notebook user's dataframe.\n\nCognitive assistant currently employs Apache\u00ae Spark (PySpark) and SKLearn (Python scikit-learn) analytics on Spark 2.1 and Python 2, optionally with feature selection, for classification and regression. \n\nThe notebook remains interactive during the optimizations. The execution counter and the kernel activity indicator reflect the activity of both the notebook user and the cognitive assistant.\n\n## Contents\n\nThis notebook has the following main sections:\n1. [Download the data](#download)\n1. [Format the data](#format)\n1. [Run the cognitive assistant optimization](#cads)\n\n<a id=\"download\"></a>\n## 1. Download the data\n\nBecause cognitive assistant is especially suited to large data sets, in the following step, you'll be retrieving a large (2.6 GB) .csv file that contains data on HIGGS boson particles. \n\nFrom the description of the data set on the UCI Web site: \"The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set.\""
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "%%bash\n#Download a portion of a popular compressed csv dataset.  Target kilobytes of data are specified after '-ge' nine or so lines below\ncat << 'EOF' > limited.sh\n# optionally you can limit the download rate to wget via --limit-rate=2m\nwget -nv  -O limited.csv.gz https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz &\nWGET_PID=$!\nwhile [ `ps $WGET_PID | wc -l ` -eq 2 ] ; do\n    du -k --apparent-size limited.csv.gz\n    sleep 1\n    #modify the target kilobyte download size after -ge below, or comment out the line below to get the whole file\n    if [ -e limited.csv.gz ] ; then if [ `du -k --apparent-size limited.csv.gz | awk '{print \\$1}'` -ge 10000  ] ; then kill -15 $WGET_PID; fi; fi\ndone\nEOF\ncat limited.sh"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "!echo \"expect an unexpected-end-of-file-message, that is ok if you specified a limited download size\"\n!bash limited.sh\n#sed will swallow incomplete last line which will likely occur in the case of a partial download\n!gunzip -c -q limited.csv.gz | sed -e 's/rarelyseen/rarelyseen/'> limited.csv\n!echo `wc -l limited.csv` ' complete lines of csv retrieved'"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "!rm -f limited.csv.gz"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"format\"></a>\n## 2. Format the data\n\nFormat the data into a Pyspark SQL dataframe consisting of a numeric `label` column and a vector `features` column."
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "df = sqlContext.read.format('com.databricks.spark.csv')\\\n  .options(header='false', inferschema='true')\\\n  .load(\"limited.csv\")"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 2.1 Remove compressed file\n\nYou can clear up space on your filesystem by removing the compressed *limited.csv.gz* file. Because of the deferred computation in Spark, the uncompressed file is read later and cannot be erased at this point."
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "!rm -f limited.csv.gz"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "allNames = [f.name for f in df.schema.fields]\n#FIRST column is the label (unlike many csv where it is the last)\nlabelName = allNames[0]\nprint 'labelName ' + labelName\nfeatureNames = [ f.name for f in df.schema.fields if f.name != labelName ]\nprint 'featureNames=' + str(featureNames)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "from pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\n\nfeatureAssembler = VectorAssembler(\n    inputCols=featureNames,\n    outputCol=\"features\")"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#create features (vector) column then select label and features column, with renaming.\nnewDF=featureAssembler.transform(df).selectExpr(labelName + \" as label\",\"features as features\")\n"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "print newDF"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"cads\"></a>\n## 3. Run the cognitive assistant optimization\n\nBefore you can use the cognitive assistant, you must import the cognitive_assistant package and then start the assistant."
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "import cognitive_assistant as cognitive"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "cognitive.assistant.startAssistant()"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 3.1 Invoke the cognitive assistant function\n\nYou invoke the cognitive assistant function by providing it the following information:\n\n-  the name of the data frame\n-  the prediction type\n    -  For a classification algorithm, you specify predictionType='CLASSIFICATION' or goalTags=\"CADS_FS_JY_CL\"\n    -  For a regression algorithm, you specify predictionType='REGRESSION' or goalTags=\"CADS_FS_JY_RG\""
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "cognitive.assistant.startOptimization(newDF, goalTags=\"CADS_FS_JY_CL\")"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 3.2 Check on the progress\n\nYou can check on the progress of optimization by running the following code. Re-invoke this as many times as you want to view current results. An error message can be expected while the optimization is starting up and you may need to wait until the process is fulling running before checking the progress."
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "cognitive.assistant.visualizeProgress()"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 3.3 Do a quick check of CPU - Optional\n\nTo further check on the progress of your optimization, you can can run the following command to display CPU activity and the number of processes that are running."
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "!top -u $USER -n 1"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "### 3.4 Stop the cognitive assistant process\n\nWhen optimization has completed or progressed to an acceptable point, the cognitive assistant can be stopped by running the following command:"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": "cognitive.assistant.stopAssistant()"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Summary\n\nYou downloaded and formatted a publicly available data set and then used the cognitive assistant to optimize the pipeline. You're probably feeling pretty good about yourself right now. You ROCK!\n\n## Authors\n\n**Peter D. Kirchner**, PhD (Electrical Engineering), is a Research Scientist persuing computer science research in machine learning and cloud computing at the IBM Thomas J. Watson Research Center. He is presently engaged in cognitive automation of data science workflow to assist data scientists, focused on cloud-based deployments and scalability.\n\n**Mike Sochka** is a content designer focusing on IBM Data Science Experience and Watson Machine Learning. "
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "***\n### References\n\nBaldi, P., P. Sadowski, and D. Whiteson. \u201cSearching for Exotic Particles in High-energy Physics with Deep Learning.\u201d Nature Communications 5 (July 2, 2014).\n\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Copyright \u00a9 2017 IBM. This notebook and its source code are released under the terms of the MIT License."
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "language": "python", 
            "name": "python2-spark21"
        }, 
        "language_info": {
            "version": "2.7.11", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "pygments_lexer": "ipython2", 
            "name": "python"
        }
    }, 
    "nbformat_minor": 1
}